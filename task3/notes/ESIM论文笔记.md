### ESIM论文笔记

#### 一、模型结构

##### 1.1 输入编码（链式LSTM）

对两个句子进行编码，得到句子的编码

##### 1.2 局部推理建模

使用**软性注意力 & 点积模型** 计算两个句子的编码（即LSTM中的隐藏状态）的相似度

1. 注意力计算及使用方式

   ![image-20220307090354628](https://s2.loli.net/2022/03/07/8rZ4YnhXcRfBDsq.png)

   ![image-20220307090332976](https://s2.loli.net/2022/03/07/3bjCJkIBg26YnOP.png)

2.  将旧ai、新ai、它们的差、哈达玛积（对应元素相乘）![image-20220307091027124](https://s2.loli.net/2022/03/07/vJ8HiUnIA9OYxXT.png)

##### 1.3 推理组合

1. 使用ma、mb再次经过一个双向LSTM层，一个全连接层，均使用ReLU作为激活函数
1. 分别经过两次池化（最大池化和平均池化），将结果拼接成一个向量
1. 最后将结果送入一个多层感知机：一个隐藏层和一个softmax输出层，隐藏层以tanh作为激活函数



#### 二、训练方式

1. 数据：SNLI语料库

   句子对的三种关系：蕴含、无关、矛盾（去除其他类别）

   测试集：development set

2. 模型

   * LSTM和字嵌入的隐藏状态维度：300
   * Dropout：应用于所有全连接层
   * 词嵌入向量：300维的Glove 840B向量（训练中会更新）
   * OOV词汇：通过高斯样本初始化

3. 训练

   * Adam优化器（第一个动量为0.9，第二个动量为0.999）
   * 初始学习率：0.0004
   * 批量大小为32
   * Tesla K40训练时间：6h（树LSTM 40h）

4. 评估

   * 准确度
   * ESIM：88%
   * 使用了TreeLSTM的HIM：88.6%



#### 三、消融实验

重要组件

* 池化层（而不是求和）
* 向量差和哈达玛积
* 使用双向LSTM进行句子编码（而不是全连接层）
* 两个句子的注意力（第二个句子的注意力更重要）



遇到的问题

1. 
2. 长度为0